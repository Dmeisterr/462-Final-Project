{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1bb6e9",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d773961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e76ee",
   "metadata": {},
   "source": [
    "## load and merge stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abebb515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 CSV files to process\n",
      "Processing HistoricalData_XLB.csv...\n",
      "  - Added 2516 rows for ticker: XLB\n",
      "Processing HistoricalData_XLU.csv...\n",
      "  - Added 2516 rows for ticker: XLU\n",
      "Processing HistoricalData_XLV.csv...\n",
      "  - Added 2516 rows for ticker: XLV\n",
      "Processing HistoricalData_XLE.csv...\n",
      "  - Added 2516 rows for ticker: XLE\n",
      "Processing HistoricalData_XLF.csv...\n",
      "  - Added 2516 rows for ticker: XLF\n",
      "Processing HistoricalData_XLP.csv...\n",
      "  - Added 2516 rows for ticker: XLP\n",
      "Processing HistoricalData_XLRE.csv...\n",
      "  - Added 2390 rows for ticker: XLRE\n",
      "Processing HistoricalData_XLK.csv...\n",
      "  - Added 2516 rows for ticker: XLK\n",
      "Processing HistoricalData_XLI.csv...\n",
      "  - Added 2516 rows for ticker: XLI\n",
      "Processing HistoricalData_XLY.csv...\n",
      "  - Added 2516 rows for ticker: XLY\n",
      "\n",
      "Final merged DataFrame shape: (2516, 11)\n",
      "Pivot table saved to: /Users/daniellott2/Library/CloudStorage/GoogleDrive-dlott@arizona.edu/My Drive/DATA 462/Final Project/data/combined_stocks.csv\n",
      "\n",
      "Date range: 2015-04-10 00:00:00 to 2025-04-09 00:00:00\n",
      "Number of dates: 2516\n",
      "Number of tickers: 10\n",
      "Tickers: XLB, XLU, XLV, XLE, XLF, XLP, XLRE, XLK, XLI, XLY\n"
     ]
    }
   ],
   "source": [
    "# Define the data path\n",
    "data_path = \"/Users/daniellott2/Library/CloudStorage/GoogleDrive-dlott@arizona.edu/My Drive/DATA 462/Final Project/data/\"\n",
    "\n",
    "# Find all CSV files with the pattern \"HistoricalData_*.csv\"\n",
    "csv_files = glob.glob(os.path.join(data_path, \"HistoricalData_*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "# Function to load and process a single CSV file\n",
    "def process_csv(file_path):\n",
    "    # Extract ticker symbol from filename\n",
    "    ticker = os.path.basename(file_path).split('_')[1].split('.')[0] if '_' in os.path.basename(file_path) else f\"Stock_{os.path.basename(file_path)[:8]}\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert Date column to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Only keep Date and Open columns\n",
    "    if 'Open' in df.columns:\n",
    "        return df[['Date', 'Open']].rename(columns={'Open': ticker})\n",
    "    else:\n",
    "        print(f\"Warning: No 'Open' column found in {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Process all CSV files and store DataFrames in a list\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        print(f\"Processing {os.path.basename(file)}...\")\n",
    "        df = process_csv(file)\n",
    "        if df is not None:\n",
    "            # Ensure numeric formatting for open price\n",
    "            df[df.columns[1]] = pd.to_numeric(df[df.columns[1]].astype(str).str.replace(',', ''), errors='coerce')\n",
    "            dfs.append(df)\n",
    "            print(f\"  - Added {df.shape[0]} rows for ticker: {df.columns[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "# Merge all DataFrames on the Date column\n",
    "if dfs:\n",
    "    # Start with the first dataframe\n",
    "    merged_df = dfs[0]\n",
    "    \n",
    "    # Merge with the rest one by one using left outer join\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on='Date', how='outer')\n",
    "    \n",
    "    print(f\"\\nFinal merged DataFrame shape: {merged_df.shape}\")\n",
    "    \n",
    "    # Sort by date\n",
    "    merged_df = merged_df.sort_values('Date')\n",
    "    \n",
    "    # Save the merged DataFrame\n",
    "    pivot_csv_path = os.path.join(data_path, \"combined_stocks.csv\")\n",
    "    merged_df.to_csv(pivot_csv_path, index=False)\n",
    "    print(f\"Pivot table saved to: {pivot_csv_path}\")\n",
    "    \n",
    "    # Display some basic information\n",
    "    print(f\"\\nDate range: {merged_df['Date'].min()} to {merged_df['Date'].max()}\")\n",
    "    print(f\"Number of dates: {merged_df['Date'].nunique()}\")\n",
    "    print(f\"Number of tickers: {len(merged_df.columns) - 1}\")  # Subtract 1 for the Date column\n",
    "    print(f\"Tickers: {', '.join(merged_df.columns[1:])}\")\n",
    "else:\n",
    "    print(\"No dataframes were successfully processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb8169",
   "metadata": {},
   "source": [
    "## add ffr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49f10fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Federal Funds Rate data...\n",
      "Expanded FFR data from monthly to daily frequency\n",
      "FFR date range: 2015-04-10 00:00:00 to 2025-04-09 00:00:00\n",
      "Combined data with FFR saved to: /Users/daniellott2/Library/CloudStorage/GoogleDrive-dlott@arizona.edu/My Drive/DATA 462/Final Project/data/combined_stocks_with_ffr.csv\n",
      "\n",
      "FFR data summary:\n",
      "Min: 0.05%, Max: 5.33%\n",
      "Mean: 1.94%, Median: 1.30%\n"
     ]
    }
   ],
   "source": [
    "def process_ffr_data(ffr_file_path):\n",
    "    \"\"\"\n",
    "    Load monthly Federal Funds Rate data and convert it to daily frequency\n",
    "    by forward-filling missing dates.\n",
    "    \"\"\"\n",
    "    # Load the FFR data\n",
    "    ffr_df = pd.read_csv(ffr_file_path)\n",
    "    \n",
    "    # Convert date column to datetime (assuming it has a date column)\n",
    "    date_col = [col for col in ffr_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    if date_col:\n",
    "        date_col = date_col[0]\n",
    "    else:\n",
    "        date_col = ffr_df.columns[0]  # Assume first column is date\n",
    "    \n",
    "    # Rename the date column to 'Date' for consistency\n",
    "    ffr_df = ffr_df.rename(columns={date_col: 'Date'})\n",
    "    \n",
    "    # Convert to datetime\n",
    "    ffr_df['Date'] = pd.to_datetime(ffr_df['Date'])\n",
    "    \n",
    "    # Rename the FFR column to 'FFR' if not already named that\n",
    "    rate_col = [col for col in ffr_df.columns if col != 'Date'][0]  # Assume the non-date column is the rate\n",
    "    ffr_df = ffr_df.rename(columns={rate_col: 'FFR'})\n",
    "    \n",
    "    # Select only Date and FFR columns\n",
    "    ffr_df = ffr_df[['Date', 'FFR']]\n",
    "    \n",
    "    # Create a date range spanning the entire period of stock data\n",
    "    date_range = pd.date_range(\n",
    "        start=merged_df['Date'].min(), \n",
    "        end=merged_df['Date'].max(), \n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame with all dates\n",
    "    full_date_df = pd.DataFrame({'Date': date_range})\n",
    "    \n",
    "    # Merge with the FFR data\n",
    "    daily_ffr = pd.merge(full_date_df, ffr_df, on='Date', how='left')\n",
    "    \n",
    "    # Forward fill missing values (use the last known FFR rate)\n",
    "    daily_ffr['FFR'] = daily_ffr['FFR'].ffill()\n",
    "    \n",
    "    return daily_ffr\n",
    "\n",
    "# Path to FFR data file - update this to your actual file path\n",
    "ffr_file_path = os.path.join(data_path, \"FFR.csv\")\n",
    "\n",
    "try:\n",
    "    print(f\"Processing Federal Funds Rate data...\")\n",
    "    daily_ffr_df = process_ffr_data(ffr_file_path)\n",
    "    print(f\"Expanded FFR data from monthly to daily frequency\")\n",
    "    print(f\"FFR date range: {daily_ffr_df['Date'].min()} to {daily_ffr_df['Date'].max()}\")\n",
    "    \n",
    "    # Merge FFR data with stock data\n",
    "    merged_df = pd.merge(merged_df, daily_ffr_df, on='Date', how='left')\n",
    "    \n",
    "    # Update the saved file with FFR data included\n",
    "    merged_csv_path = os.path.join(data_path, \"combined_stocks_with_ffr.csv\")\n",
    "    merged_df.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Combined data with FFR saved to: {merged_csv_path}\")\n",
    "    \n",
    "    # Display some basic information about the FFR data\n",
    "    print(f\"\\nFFR data summary:\")\n",
    "    print(f\"Min: {merged_df['FFR'].min():.2f}%, Max: {merged_df['FFR'].max():.2f}%\")\n",
    "    print(f\"Mean: {merged_df['FFR'].mean():.2f}%, Median: {merged_df['FFR'].median():.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing FFR data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4da7e8",
   "metadata": {},
   "source": [
    "## add mortgage rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a704d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mortgage rate data...\n",
      "Expanded mortgage rate data from weekly to daily frequency\n",
      "Mortgage date range: 2015-04-10 00:00:00 to 2025-04-09 00:00:00\n",
      "\n",
      "Mortgage rate data summary:\n",
      "Min: 2.65%, Max: 7.79%\n",
      "Mean: 4.57%, Median: 4.02%\n",
      "Combined data with FFR and mortgage rates saved to: /Users/daniellott2/Library/CloudStorage/GoogleDrive-dlott@arizona.edu/My Drive/DATA 462/Final Project/data/combined_stocks_with_ffr_mortgage.csv\n"
     ]
    }
   ],
   "source": [
    "def process_mortgage_rates(mortgage_file_path):\n",
    "    \"\"\"\n",
    "    Load weekly mortgage rate data and convert it to daily frequency\n",
    "    by forward-filling missing dates.\n",
    "    \"\"\"\n",
    "    print(f\"Processing mortgage rate data...\")\n",
    "    \n",
    "    # Load the mortgage rate data\n",
    "    mortgage_df = pd.read_csv(mortgage_file_path)\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    mortgage_df['observation_date'] = pd.to_datetime(mortgage_df['observation_date'])\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    mortgage_df = mortgage_df.rename(columns={'observation_date': 'Date', 'MORTGAGE30US': 'Mortgage30Y'})\n",
    "    \n",
    "    # Create a date range spanning the entire period of stock data\n",
    "    date_range = pd.date_range(\n",
    "        start=merged_df['Date'].min(),\n",
    "        end=merged_df['Date'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame with all dates\n",
    "    full_date_df = pd.DataFrame({'Date': date_range})\n",
    "    \n",
    "    # Merge with the mortgage data\n",
    "    daily_mortgage = pd.merge(full_date_df, mortgage_df, on='Date', how='left')\n",
    "    \n",
    "    # Forward fill missing values (use the last known mortgage rate)\n",
    "    daily_mortgage['Mortgage30Y'] = daily_mortgage['Mortgage30Y'].ffill()\n",
    "    \n",
    "    print(f\"Expanded mortgage rate data from weekly to daily frequency\")\n",
    "    print(f\"Mortgage date range: {daily_mortgage['Date'].min()} to {daily_mortgage['Date'].max()}\")\n",
    "    \n",
    "    return daily_mortgage\n",
    "\n",
    "# Path to mortgage rate data file\n",
    "mortgage_file_path = os.path.join(data_path, \"MORTGAGE30US.csv\")\n",
    "\n",
    "try:\n",
    "    # Process mortgage rates data\n",
    "    daily_mortgage_df = process_mortgage_rates(mortgage_file_path)\n",
    "    \n",
    "    # Merge mortgage data with the existing dataset\n",
    "    merged_df = pd.merge(merged_df, daily_mortgage_df, on='Date', how='left')\n",
    "    \n",
    "    # Display some basic information about the mortgage data\n",
    "    print(f\"\\nMortgage rate data summary:\")\n",
    "    print(f\"Min: {merged_df['Mortgage30Y'].min():.2f}%, Max: {merged_df['Mortgage30Y'].max():.2f}%\")\n",
    "    print(f\"Mean: {merged_df['Mortgage30Y'].mean():.2f}%, Median: {merged_df['Mortgage30Y'].median():.2f}%\")\n",
    "    \n",
    "    # Update the saved file with mortgage data included\n",
    "    final_csv_path = os.path.join(data_path, \"combined_stocks_with_ffr_mortgage.csv\")\n",
    "    merged_df.to_csv(final_csv_path, index=False)\n",
    "    print(f\"Combined data with FFR and mortgage rates saved to: {final_csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing mortgage rate data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf580ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add ZHVI data (home price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca298175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ZHVI housing price data...\n",
      "Expanded ZHVI data from monthly to daily frequency\n",
      "ZHVI date range: 2015-04-10 00:00:00 to 2025-04-09 00:00:00\n",
      "\n",
      "Florida ZHVI data summary:\n",
      "Min: $175102.09, Max: $394769.17\n",
      "Mean: $279632.43, Median: $249868.93\n",
      "\n",
      "Arizona ZHVI data summary:\n",
      "Min: $196768.81, Max: $456005.45\n",
      "Mean: $315963.88, Median: $283642.18\n",
      "Combined data with FFR, mortgage rates, and ZHVI saved to: /Users/daniellott2/Library/CloudStorage/GoogleDrive-dlott@arizona.edu/My Drive/DATA 462/Final Project/data/combined_stocks_with_ffr_mortgage_zhvi.csv\n"
     ]
    }
   ],
   "source": [
    "def process_zhvi_data(zhvi_file_path, states_to_include=['Florida', 'Arizona']):\n",
    "    \"\"\"\n",
    "    Load monthly ZHVI (Zillow Home Value Index) data for specific states and \n",
    "    convert it to daily frequency by forward-filling missing dates.\n",
    "    \"\"\"\n",
    "    print(f\"Processing ZHVI housing price data...\")\n",
    "    \n",
    "    # Load the ZHVI data\n",
    "    zhvi_df = pd.read_csv(zhvi_file_path)\n",
    "    \n",
    "    # Filter for specified states only\n",
    "    zhvi_df = zhvi_df[zhvi_df['RegionName'].isin(states_to_include)]\n",
    "    \n",
    "    if len(zhvi_df) == 0:\n",
    "        raise ValueError(f\"No data found for states: {states_to_include}\")\n",
    "        \n",
    "    # Melt the dataframe to convert from wide to long format\n",
    "    # First, identify date columns (they should be in YYYY-MM-DD format)\n",
    "    date_columns = [col for col in zhvi_df.columns if '-' in col]\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    zhvi_melted = pd.melt(\n",
    "        zhvi_df,\n",
    "        id_vars=['RegionName'],\n",
    "        value_vars=date_columns,\n",
    "        var_name='Date',\n",
    "        value_name='ZHVI'\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime\n",
    "    zhvi_melted['Date'] = pd.to_datetime(zhvi_melted['Date'])\n",
    "    \n",
    "    # Create a dataframe for each state\n",
    "    state_dfs = {}\n",
    "    for state in states_to_include:\n",
    "        state_data = zhvi_melted[zhvi_melted['RegionName'] == state].copy()\n",
    "        \n",
    "        # Create column name with state prefix\n",
    "        column_name = f\"{state}_ZHVI\"\n",
    "        \n",
    "        # Create a date range spanning the entire period of stock data\n",
    "        date_range = pd.date_range(\n",
    "            start=merged_df['Date'].min(),\n",
    "            end=merged_df['Date'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        # Create a DataFrame with all dates\n",
    "        full_date_df = pd.DataFrame({'Date': date_range})\n",
    "        \n",
    "        # Merge with the ZHVI data\n",
    "        state_df = pd.merge(full_date_df, \n",
    "                           state_data[['Date', 'ZHVI']], \n",
    "                           on='Date', \n",
    "                           how='left')\n",
    "        \n",
    "        # Forward fill missing values\n",
    "        state_df['ZHVI'] = state_df['ZHVI'].ffill()\n",
    "        \n",
    "        # Rename for clarity\n",
    "        state_df = state_df.rename(columns={'ZHVI': column_name})\n",
    "        \n",
    "        state_dfs[state] = state_df\n",
    "    \n",
    "    # Merge all state dataframes together\n",
    "    result_df = state_dfs[states_to_include[0]]\n",
    "    for state in states_to_include[1:]:\n",
    "        result_df = pd.merge(result_df, state_dfs[state], on='Date', how='outer')\n",
    "    \n",
    "    print(f\"Expanded ZHVI data from monthly to daily frequency\")\n",
    "    print(f\"ZHVI date range: {result_df['Date'].min()} to {result_df['Date'].max()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Path to ZHVI data file\n",
    "zhvi_file_path = os.path.join(data_path, \"ZHVI.csv\")\n",
    "\n",
    "try:\n",
    "    # Process ZHVI data\n",
    "    daily_zhvi_df = process_zhvi_data(zhvi_file_path, states_to_include=['Florida', 'Arizona'])\n",
    "    \n",
    "    # Merge ZHVI data with the existing dataset\n",
    "    merged_df = pd.merge(merged_df, daily_zhvi_df, on='Date', how='left')\n",
    "    \n",
    "    # Display some basic information about the ZHVI data\n",
    "    print(f\"\\nFlorida ZHVI data summary:\")\n",
    "    print(f\"Min: ${merged_df['Florida_ZHVI'].min():.2f}, Max: ${merged_df['Florida_ZHVI'].max():.2f}\")\n",
    "    print(f\"Mean: ${merged_df['Florida_ZHVI'].mean():.2f}, Median: ${merged_df['Florida_ZHVI'].median():.2f}\")\n",
    "    \n",
    "    print(f\"\\nArizona ZHVI data summary:\")\n",
    "    print(f\"Min: ${merged_df['Arizona_ZHVI'].min():.2f}, Max: ${merged_df['Arizona_ZHVI'].max():.2f}\")\n",
    "    print(f\"Mean: ${merged_df['Arizona_ZHVI'].mean():.2f}, Median: ${merged_df['Arizona_ZHVI'].median():.2f}\")\n",
    "    \n",
    "    # Update the saved file with ZHVI data included\n",
    "    final_csv_path = os.path.join(data_path, \"combined_stocks_with_ffr_mortgage_zhvi.csv\")\n",
    "    merged_df.to_csv(final_csv_path, index=False)\n",
    "    print(f\"Combined data with FFR, mortgage rates, and ZHVI saved to: {final_csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing ZHVI data: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
